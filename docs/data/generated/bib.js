define({ entries : {
    "cai2025_rank_score": {
        "abstract": "In recent years, large language models (LLMs) achieve remarkable success across a variety of tasks. However, their potential in the domain of Automated Essay Scoring (AES) remains largely underexplored. Moreover, compared to English data, the methods for Chinese AES is not well developed. In this paper, we propose Rank-Then-Score (RTS), a fine-tuning frame- work based on large language models to enhance their essay scoring capabilities. Specifically, we fine-tune the ranking model (Ranker) with feature-enriched data, and then feed the output of the ranking model, in the form of a candidate score set, with the essay content into the scoring model (Scorer) to produce the final score. Experimental results on two benchmark datasets, HSK and ASAP, demonstrate that RTS consistently outperforms the direct prompting (Vanilla) method in terms of average QWK across all LLMs and datasets, and achieves the best performance on Chinese essay scoring using the HSK dataset.",
        "archiveprefix": "arXiv",
        "author": "Yida Cai and Kun Liang and Sanwoo Lee and Qinghan Wang and Yunfang Wu",
        "doi": "https://doi.org/10.48550/arXiv.2504.05736",
        "eprint": "2504.05736",
        "img": "cai2025_rank_score",
        "journal": "arXiv preprint",
        "keywords": "type:preprint, model:ChatGPT, dataset:ASAP, dataset:HSK, evaluation:QWK, AES, Rank-Then-Score, prompt design, human-machine consistency",
        "primaryclass": "cs.CL",
        "series": "arXiv",
        "title": "Rank-Then-Score: Enhancing Large Language Models for Automated Essay Scoring",
        "type": "article",
        "url": "https://arxiv.org/abs/2504.05736",
        "year": "2025"
    },
    "lee2024_cot_scoring": {
        "(acc": "0.60). However, CoT prompting paired with contextual item stems and rubrics proved to be a significant contributor to scoring accuracy (13.44% increase for zero-shot; 3.7% increase for few-shot). We found a more balanced accuracy across different proficiency categories when CoT was used with a scoring rubric, highlighting the importance of domain-specific reasoning in enhancing the effectiveness of LLMs in scoring tasks. We also found that GPT-4 demonstrated superior performance over GPT-3.5 in various scoring tasks when combined with the single-call greedy sampling or ensemble voting nucleus sampling strategy, showing 8.64% difference. Particularly, the single-call greedy sampling strategy with GPT-4 outperformed other approaches. This study also demonstrates the potential of LLMs in facilitating explainable and interpretable automatic scoring, emphasizing that CoT enhances accuracy and transparency, particularly when used with item stem and scoring rubrics.",
        "abstract": "This study investigates the application of large language models (LLMs), specifically GPT-3.5 and GPT-4, with Chain-of-Though (CoT) in the automatic scoring of student-written responses to science assessments. We focused on overcoming the challenges of accessibility, technical complexity, and lack of explainability that have previously limited the use of artificial intelligence-based automatic scoring tools among researchers and educators. With a testing dataset comprising six assessment tasks (three binomial and three trinomial) with 1,650 student responses, we employed six prompt engineering strategies to automatically score student responses. The six strategies combined zero-shot or few-shot learning with CoT, either alone or alongside item stem and scoring rubrics, developed based on a novel approach, WRVRT (prompt writing, reviewing, validating, revising, and",
        "author": "Gyeong-Geon Lee and Ehsan Latif and Xuansheng Wu and Ninghao Liu and Xiaoming Zhai",
        "doi": "https://doi.org/10.1016/j.caeai.2024.100213",
        "img": "lee2024_cot_scoring",
        "issn": "2666-920X",
        "journal": "Computers and Education: Artificial Intelligence",
        "keywords": "type:journal article, model:GPT-3.5, model:GPT-4, dataset:custom dataset, evaluation:accuracy, AES, chain-of-thought, WRVRT, prompt design, rubric-based scoring, human-machine consistency",
        "pages": "100213",
        "series": "CAEAI",
        "testing). results indicated that few-shot (acc": "0.67) outperformed zero-shot learning (acc increase. CoT, when used without item stem and scoring rubrics, did not significantly affect scoring accuracy",
        "title": "Applying large language models and chain-of-thought for automatic scoring",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/pii/S2666920X24000146",
        "volume": "6",
        "year": "2024"
    },
    "mizumoto2023_gpt_toefl11": {
        "abstract": "The widespread adoption of ChatGPT, an AI language model, has the potential to bring about significant changes to the research, teaching, and learning of foreign languages. The present study aims to leverage this technology to perform automated essay scoring (AES) and evaluate its reliability and accuracy. Specifically, we utilized the GPT-3 text-davinci-003 model to automatically score all 12,100 essays contained in the ETS Corpus of Non-Native Written English (TOEFL11) and compared these scores to benchmark levels. The study also explored the extent to which lin- guistic features influence AES with GPT. The results showed that AES using GPT has a certain level of accuracy and reliability and could provide valuable support for human evaluations. Furthermore, the analysis revealed that utilizing linguistic features could enhance the accuracy of the scoring. These findings suggest that AI language models, such as ChatGPT, can be effectively utilized as AES tools, potentially revolutionizing methods of writing evaluation and feedback in both research and practice. The paper concludes by discussing the practical implications of using GPT for AES and exploring prospective future considerations.",
        "author": "Atsushi Mizumoto and Masaki Eguchi",
        "doi": "https://doi.org/10.1016/j.rmal.2023.100050",
        "img": "mizumoto2023_gpt_toefl11",
        "issn": "2772-7661",
        "journal": "Research Methods in Applied Linguistics",
        "keywords": "type:journal article, model:GPT-3.5, dataset:TOEFL, evaluation:QWK, evaluation:LOOIC, evaluation:pseudo-R\u00b2, AES, linguistic feature, human-machine consistency",
        "number": "2",
        "pages": "100050",
        "series": "RMAL",
        "title": "Exploring the potential of using an AI language model for automated essay scoring",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/pii/S2772766123000101",
        "volume": "2",
        "year": "2023"
    },
    "pack2024_llm_aes": {
        "abstract": "Advancements in generative AI, such as large language models (LLMs), may serve as a potential solution to the burdensome task of essay grading often faced by language education teachers. Yet, the validity and reliability of leveraging LLMs for automatic essay scoring (AES) in language education is not well understood. To address this, we evaluated the cross-sectional and longitudinal validity and reliability of four prominent LLMs, Google's PaLM 2, Anthropic's Claude 2, and OpenAI's GPT-3.5 and GPT-4, for the AES of English language learners' writing. 119 essays taken from an English language placement test were assessed twice by each LLM, on two separate oc casions, as well as by a pair of human raters. GPT-4 performed the best, demonstrating excellent intrarater reliability and good validity. All models, with the exception of GPT-3.5, improved over time in their intrarater reliability. The interrater reliability of GPT-3.5 and GPT-4, however, decreased slightly over time. These findings indicate that some models perform better than others in AES and that all models are subject to fluctuations in their performance. We discuss potential reasons for such variability, and offer suggestions for prospective ave nues of research.",
        "author": "Austin Pack and Alex Barrett and Juan Escalante",
        "doi": "https://doi.org/10.1016/j.caeai.2024.100234",
        "img": "pack2024_llm_aes",
        "issn": "2666-920X",
        "journal": "Computers and Education: Artificial Intelligence",
        "keywords": "type:journal article, model:GPT-3.5, model:GPT-4, model:Claude, model:PaLM-2, dataset:custom dataset, evaluation:ICC, AES, rubric-based scoring, human-machine consistency",
        "pages": "100234",
        "series": "CAEAI",
        "title": "Large language models and automated essay scoring of English language learner writing: Insights into validity and reliability",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/pii/S2666920X24000353",
        "volume": "6",
        "year": "2024"
    },
    "qinthara2025_reliability_aes": {
        "abstract": "Maintaining consistency in automated essay scoring is essential to guarantee fair and dependable assessments. This study investigates consistency and provides a comparative analysis of open-source and proprietary large language models (LLMs) for automated essay scoring (AES). The study utilized student essays, each assessed five times to measure both intrarater (using intraclass coefficient and repeatability coefficient) and interrater (concordance correlation coefficient) reliability across several models: GPT-4, GPT-4o, GPT-4o mini, GPT-3.5 Turbo, Gemini 1.5 Flash, and LLaMa 3.1 70B. Essays and marking criteria are used for prompt construction and sent to each large language model to obtain score outputs. Results indicate that the scores generated by GPT-4o closely align with human assessments, demonstrating fair agreement across repeated measures. Specifically, GPT-4o exhibits slightly higher concordance correlation coefficients (CCC) than GPT-4o mini, indicating superior agreement with human scores. However, qualitatively, it can be observed that all LLM models are not as consistent in terms of their scoring rationale/evaluation. Our study results indicate that the challenges currently faced in automated essay scoring with large language models need to be analyzed not only from a quantitative perspective but also qualitatively. Additionally, we utilize more sophisticated prompting methods and address the inconsistencies observed in the initial measurements. Despite the purported reliability of some models within our study, the selection of LLMs should be considered thoroughly during practical implementations for an AES.",
        "author": "Siti Bealinda Qinthara and Xin Fei Tan and Sasa Arsovski",
        "doi": "10.37074/jalt.2025.8.1.21",
        "img": "qinthara2025_reliability_aes",
        "issn": "2591-801X",
        "journal": "Journal of Applied Learning \\& Teaching",
        "keywords": "type:journal article, model:GPT-3.5, model:GPT-4, model:GPT-4o, model:GPT-4o mini, model:Gemini, model:LLaMA, dataset:custom dataset, evaluation:ICC, evaluation:CCC, AES, human-machine consistency",
        "note": "open access",
        "number": "1",
        "pages": "67--75",
        "series": "JALT",
        "title": "Educational Justice: Reliability and Consistency of LLMs for AES and Its Implications",
        "type": "article",
        "url": "https://doi.org/10.37074/jalt.2025.8.1.21",
        "volume": "8",
        "year": "2025"
    },
    "song2024_openllm_aes_aer": {
        "abstract": "Manually scoring and revising student essays has long been a time-consuming task for educators. With the rise of natural language processing techniques, automated essay scoring (AES) and automated essay revising (AER) have emerged to alleviate this burden. However, current AES and AERmodels require large amounts of training data and lack generalizability, which makes them hard to implement in daily teaching activities. Moreover, online sites offering AES and AER services charge high fees and have security issues uploading student content. In light of these challenges and recognizing the advancements in large language models (LLMs), we aim to fill these research gaps by analyzing the performance of open-source LLMs when accomplishing AES",
        "and aer tasks. using a human-scored essay dataset (n": "600) collected in an online assessment, we implemented zero-shot, few- shot, and p-tuningAESmethods based on theLLMsand conducted a human-machine consistency check. We conducted a similarity test and a score difference test for the results of AER with LLMs support. The human-machine consistency check result shows that theperformanceofopen-sourceLLMswith a10Bparameter size in the AES task is close to that of some deep-learning baseline models, and it can be improved by integrating the comment with the score into the shot or training continuous prompts. The similarity test and score difference test results show that open-source LLMs can effectively accomplish the AER task, improving the quality of the essays while ensuring that the revision results are similar to the original essays. This study reveals a practical path to cost-effectively, time-efficiently, and content-safely assisting teachers with student essay scoring and revising using open-source LLMs.",
        "author": "Yishen Song and Qianta Zhu and Huaibo Wang and Qinhua Zheng",
        "doi": "10.1109/TLT.2024.3396873",
        "img": "song2024_openllm_aes_aer",
        "issn": "1939-1382",
        "journal": "IEEE Transactions on Learning Technologies",
        "keywords": "type:journal article, model:ChatGLM, model:Baichuan, dataset:custom dataset, evaluation:QWK, AES, AER, p-tuning, rubric-based scoring, prompt design, human-machine consistency",
        "pages": "1880--1890",
        "series": "TLT",
        "title": "Automated Essay Scoring and Revising Based on Open-Source Large Language Models",
        "type": "article",
        "volume": "17",
        "year": "2024"
    },
    "su2025_essayjudge_benchmark": {
        "abstract": "Automated Essay Scoring (AES) plays a crucial role in educational assessment by providing scalable and consistent evaluations of writing tasks. However, traditional AES systems face three major challenges: \u2776 reliance on handcrafted features that limit generalizability, \u2777 difficulty in capturing fine-grained traits like coherence and argumentation, and \u2778 inability to handle multimodal contexts. In the era of Multimodal Large Language Models (MLLMs), we propose ESSAYJUDGE, the first multimodal benchmark to evaluate AES capabilities across lexical-, sentence-, and discourse-level traits. By leveraging MLLMs' strengths in trait-specific scoring and multi-modal context understanding, ESSAYJUDGE aims to offer precise, context-rich evaluations without manual feature engineering, addressing longstanding AES limitations. Our experiments with 18 representative MLLMs reveal gaps in AES performance compared to human evaluation, particularly in discourse-level traits, highlighting the need for further advancements in MLLM-based AES research. Our dataset and code will be available upon acceptance.",
        "archiveprefix": "arXiv",
        "author": "Jiamin Su and Yibo Yan and Fangteng Fu and Han Zhang and Jingheng Ye and Xiang Liu and Jiahao Huo and Huiyu Zhou and Xuming Hu",
        "doi": "https://doi.org/10.48550/arXiv.2502.11916",
        "eprint": "2502.11916",
        "img": "su2025_essayjudge_benchmark",
        "journal": "arXiv preprint",
        "keywords": "type:preprint, dataset:EssayJudge, evaluation:accuracy, AES, multimodal LLMs, model:GPT-4o, model:GPT-4o mini, model:Gemini, model:LLaMA, model:Claude, trait-level scoring, human-machine consistency",
        "primaryclass": "cs.CL",
        "series": "arXiv",
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "type": "article",
        "url": "https://arxiv.org/abs/2502.11916",
        "year": "2025"
    },
    "wang2024_llm_validity": {
        "abstract": "This study investigates the potential of large language models (LLMs), specifically GPT-4, for automated writing scoring and feedback generation. Em ploying a mixed-methods approach, the research evaluates the accuracy and reli ability of GPT-4 in predicting essay scores and the quality of its generated feed back. The results demonstrate a high level of agreement between GPT-4 scores and human raters, as evidenced by the confusion matrix and Quadratic Weighted Kappa metric. Qualitative analysis of GPT-4 feedback suggests its ability to pro vide constructive and comprehensive suggestions for improving student writing. However, there are still limitations surrounding LLM-based automated scoring and feedbacks. Thus, this study proposes the use of LLM-based systems as form ative assessment tools to complement human judgment.",
        "author": "Shan Wang",
        "booktitle": "Proceedings of the 2024 5th International Conference on Education, Knowledge and Information Management (ICEKIM 2024)",
        "doi": "10.2991/978-94-6463-502-7_116",
        "img": "wang2024_llm_validity",
        "isbn": "978-94-6463-502-7",
        "issn": "2589-4900",
        "journal": "ICEKIM 2024: Int. Conf. on Education, Knowledge and Information Management",
        "keywords": "type:conference paper, model:GPT-4, dataset:ASAP, evaluation:QWK, AES, AER, rubric-based scoring, human-machine consistency",
        "pages": "1091--1098",
        "publisher": "Atlantis Press",
        "series": "ICEKIM",
        "title": "Investigating the Potential of Large Language Models for Automated Writing Scoring",
        "type": "article",
        "url": "https://doi.org/10.2991/978-94-6463-502-7_116",
        "year": "2024"
    },
    "xu2024_slr_aes": {
        "abstract": "Artificial intelligence technology is becoming increasingly essential to education. The outbreak ofCOVID-19 in recent years has led many schools to launch online education. Automated online assessments have become a hot topic of interest, and an increasing number of researchers are studying Automated Essay Scoring (AES). This work seeks to summarise the characteristics of current AES systems used in English writing assessment, identify their strengths and weaknesses, and finally, analyse the limits of recent studies and research trends. Search strings were used to retrieve papers on AES systems from 2018 to 2023 from four databases, 104 of which were chosen to be potential to address the posed research aims after study selection and quality evaluation. It is concluded that the existing AES systems, although achieving good results in terms of accuracy in specific contexts, are unable to meet the needs of teachers and students in real teaching scenarios. The improvements of these systems relate to the scalability of the system for assessing different topics or styles of the essays, the accuracy of the model's predicted scores, as well as the reliability of outcomes: improving the robustness of AES models with some adversarial inputs, the richness of AES system functionality, and the development of AES assist tools.",
        "author": "Wenbo Xu and Rohana Mahmud and Wai Lam Hoo",
        "doi": "10.1109/ACCESS.2024.3399163",
        "img": "xu2024_slr_aes",
        "issn": "2169-3536",
        "journal": "IEEE Access",
        "keywords": "type:review article, model:GPT-4, dataset:TOEFL, dataset:FCE, dataset:ASAP, evaluation:QWK, evaluation:accuracy, AES, human-machine consistency",
        "pages": "77639--77657",
        "series": "ACCESS",
        "title": "A Systematic Literature Review: Are Automated Essay Scoring Systems Competent in Real-Life Education Scenarios?",
        "type": "article",
        "volume": "12",
        "year": "2024"
    },
    "yavuz2025_llm_rubric": {
        "abstract": "This study investigates the validity and reliability of generative large language models (LLMs), specifically ChatGPT and Google's Bard, in grading student essays in higher education based on an analytical grading rubric. A total of 15 experienced English as a foreign language (EFL) instructors and two LLMs were asked to evaluate three student essays of varying quality. The grading scale comprised five domains: grammar, content, organization, style & expression and mechanics. The results revealed that fine-tuned ChatGPT model demonstrated a very high level of reliability with an intraclass correlation (ICC) score of 0.972, Default ChatGPT model exhibited an ICC score of 0.947 and Bard showed a substantial level of reliability with an ICC score of 0.919. Additionally, a significant overlap was observed in certain domains when comparing the grades assigned by LLMs and human raters. In conclusion, the findings suggest that while LLMs demonstrated a notable consistency and potential for grading competency, further fine-tuning and adjustment are needed for a more nuanced understanding of non-objective essay criteria. The study not only offers insights into the potential use of LLMs in grading student essays but also highlights the need for continued development and research.",
        "author": "Fatih Yavuz and \u00d6zg\u00fcr \u00c7elik and Gamze Yava\u015f \u00c7elik",
        "doi": "https://doi.org/10.1111/bjet.13494",
        "eprint": "https://bera-journals.onlinelibrary.wiley.com/doi/pdf/10.1111/bjet.13494",
        "img": "yavuz2025_llm_rubric",
        "journal": "British Journal of Educational Technology",
        "keywords": "type:journal article, model:ChatGPT, model:Bard, dataset:custom dataset, evaluation:ICC, evaluation:QWK, AES, human-machine consistency",
        "number": "1",
        "pages": "150--166",
        "series": "BJET",
        "title": "Utilizing large language models for EFL essay grading: An examination of reliability and validity in rubric-based assessments",
        "type": "article",
        "url": "https://bera-journals.onlinelibrary.wiley.com/doi/abs/10.1111/bjet.13494",
        "volume": "56",
        "year": "2025"
    }
}});